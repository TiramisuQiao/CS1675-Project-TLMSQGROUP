{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ℹ️ **Info**\n",
    "* **forked original great work kernels**\n",
    "\n",
    "    * https://www.kaggle.com/code/ambrosm/adc24-intro-inference\n",
    "    * https://www.kaggle.com/code/hugowjd/neurips-ariel-2024-starter\n",
    "    * https://www.kaggle.com/code/xiaocao123/neurips-ariel-2024-starter?scriptVersionId=193156800\n",
    "    * https://www.kaggle.com/code/bingyuniu/neurips-ariel-2024-starter-withdifferentparametr\n",
    "\n",
    "* **2024/08/26 My Additional**\n",
    "    * percentile FE add.\n",
    "```\n",
    "np.percentile(window, 5, axis=1),\n",
    "np.percentile(window, 10, axis=1),\n",
    "np.percentile(window, 15, axis=1),\n",
    "np.percentile(window, 20, axis=1),\n",
    "np.percentile(window, 25, axis=1),\n",
    "np.percentile(window, 30, axis=1),\n",
    "np.percentile(window, 35, axis=1),\n",
    "np.percentile(window, 40, axis=1),\n",
    "np.percentile(window, 60, axis=1),\n",
    "np.percentile(window, 65, axis=1),\n",
    "np.percentile(window, 70, axis=1),\n",
    "np.percentile(window, 75, axis=1),\n",
    "np.percentile(window, 80, axis=1),\n",
    "np.percentile(window, 85, axis=1),\n",
    "np.percentile(window, 90, axis=1),\n",
    "np.percentile(window, 95, axis=1),\n",
    "np.median(window, axis=1),\n",
    "np.var(window, axis=1),\n",
    "```\n",
    "\n",
    "* **2024/08/29 My Additional**\n",
    "    * add FE\n",
    "```\n",
    "f_sliding_features2 = sliding_window_features(f_raw, 100, 10)\n",
    "a_sliding_features2 = sliding_window_features(a_raw, 100, 10)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:29:44.112891Z",
     "iopub.status.busy": "2024-12-28T10:29:44.112500Z",
     "iopub.status.idle": "2024-12-28T10:29:51.544305Z",
     "shell.execute_reply": "2024-12-28T10:29:51.543046Z",
     "shell.execute_reply.started": "2024-12-28T10:29:44.112847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "## Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:29:51.547984Z",
     "iopub.status.busy": "2024-12-28T10:29:51.547404Z",
     "iopub.status.idle": "2024-12-28T10:29:51.559188Z",
     "shell.execute_reply": "2024-12-28T10:29:51.558003Z",
     "shell.execute_reply.started": "2024-12-28T10:29:51.547950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "PATH = \"/kaggle/input/ariel-data-challenge-2024\"\n",
    "def load_signal_data(planet_id, dataset, instrument, img_size):\n",
    "    file_path = f'{PATH}/{dataset}/{planet_id}/{instrument}_signal.parquet'\n",
    "    signal = pl.read_parquet(file_path)\n",
    "    mean_signal = signal.cast(pl.Int32).sum_horizontal().cast(pl.Float32).to_numpy() / img_size # mean over the 32*32 pixels\n",
    "    net_signal = mean_signal[1::2] - mean_signal[0::2]\n",
    "    return net_signal\n",
    "\n",
    "def read_and_preprocess(dataset, planet_ids, instrument = \"AIRS-CH0\"):\n",
    "    \"\"\"Read the files for all planet_ids and extract the time series.\n",
    "    Parameters\n",
    "    dataset: 'train' or 'test'\n",
    "    planet_ids: list of planet ids\n",
    "    instrument: the instrument of observation, 'AIRS-CH0' or 'FGS1', default to 'AIRS-CH0'\n",
    "    Returns\n",
    "    dataframe with one row per planet_id and 67500 values per row for FGS1 and 5624 for AIRS-CH0\n",
    "    \"\"\"\n",
    "    img_size = 1024 if instrument == \"FGS1\" else 32*356\n",
    "    column_num = 67500 if instrument == 'FGS1' else 5625\n",
    "    raw_train = np.full((len(planet_ids), column_num), np.nan, dtype=np.float32)\n",
    "    for i, planet_id in tqdm(list(enumerate(planet_ids))):\n",
    "        raw_train[i] = load_signal_data(planet_id, dataset, instrument, img_size)\n",
    "    return raw_train\n",
    "\n",
    "def feature_engineering(f_raw, a_raw, adc_info, window_size=60, step_size=15):\n",
    "    \"\"\"Create a dataframe with combined features from the raw data, including sliding window and time-series statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    f_raw: ndarray of shape (n_planets, 67500)\n",
    "    a_raw: ndarray of shape (n_planets, 5625)\n",
    "    window_size: int, size of the sliding window for time-series statistics\n",
    "    step_size: int, step size for the sliding window\n",
    "    \n",
    "    Return value:\n",
    "    df: DataFrame of shape (n_planets, several features)\n",
    "    \"\"\"\n",
    "    f_obscured = f_raw[:, 23500:44000].mean(axis=1)\n",
    "    f_unobscured = (f_raw[:, :20500].mean(axis=1) + f_raw[:, 47000:].mean(axis=1)) / 2\n",
    "    f_relative_reduction = (f_unobscured - f_obscured) / f_unobscured\n",
    "    f_std_dev = f_raw.std(axis=1)\n",
    "    f_signal_to_noise = f_unobscured / f_std_dev\n",
    "\n",
    "    a_obscured = a_raw[:, 1958:3666].mean(axis=1)\n",
    "    a_unobscured = (a_raw[:, :1708].mean(axis=1) + a_raw[:, 3916:].mean(axis=1)) / 2\n",
    "    a_relative_reduction = (a_unobscured - a_obscured) / a_unobscured\n",
    "    a_std_dev = a_raw.std(axis=1)\n",
    "    a_signal_to_noise = a_unobscured / a_std_dev\n",
    "\n",
    "    f_variance = f_raw.var(axis=1)\n",
    "    a_variance = a_raw.var(axis=1)\n",
    "    \n",
    "    f_skewness = pd.DataFrame(f_raw).skew(axis=1).values\n",
    "    a_skewness = pd.DataFrame(a_raw).skew(axis=1).values\n",
    "\n",
    "    f_kurtosis = pd.DataFrame(f_raw).kurtosis(axis=1).values\n",
    "    a_kurtosis = pd.DataFrame(a_raw).kurtosis(axis=1).values\n",
    "    \n",
    "    f_half_obscured1 = f_raw[:, 20500:23500].mean(axis=1)\n",
    "    f_half_obscured2 = f_raw[:, 44000:47000].mean(axis=1)\n",
    "    f_half_reduction1 = (f_unobscured - f_half_obscured1) / f_unobscured\n",
    "    f_half_reduction2 = (f_unobscured - f_half_obscured2) / f_unobscured\n",
    "\n",
    "    a_half_obscured1 = a_raw[:, 1708:1958].mean(axis=1)\n",
    "    a_half_obscured2 = a_raw[:, 3666:3916].mean(axis=1)\n",
    "    a_half_reduction1 = (a_unobscured - a_half_obscured1) / a_unobscured\n",
    "    a_half_reduction2 = (a_unobscured - a_half_obscured2) / a_unobscured\n",
    "\n",
    "    # Sliding window features\n",
    "    def sliding_window_features(data, window_size, step_size):\n",
    "        features = []\n",
    "        max_index = data.shape[1]\n",
    "        for start in range(0, max_index - window_size + 1, step_size):\n",
    "            end = start + window_size\n",
    "            window = data[:, start:end]\n",
    "            features.append([\n",
    "                np.mean(window, axis=1),\n",
    "                np.std(window, axis=1),\n",
    "                np.min(window, axis=1),\n",
    "                np.max(window, axis=1),\n",
    "                np.percentile(window, 5, axis=1),\n",
    "                np.percentile(window, 10, axis=1),\n",
    "                np.percentile(window, 15, axis=1),\n",
    "                np.percentile(window, 20, axis=1),\n",
    "                np.percentile(window, 25, axis=1),\n",
    "                np.percentile(window, 30, axis=1),\n",
    "                np.percentile(window, 35, axis=1),\n",
    "                np.percentile(window, 40, axis=1),\n",
    "                np.percentile(window, 60, axis=1),\n",
    "                np.percentile(window, 65, axis=1),\n",
    "                np.percentile(window, 70, axis=1),\n",
    "                np.percentile(window, 75, axis=1),\n",
    "                np.percentile(window, 80, axis=1),\n",
    "                np.percentile(window, 85, axis=1),\n",
    "                np.percentile(window, 90, axis=1),\n",
    "                np.percentile(window, 95, axis=1),\n",
    "                np.median(window, axis=1),\n",
    "                np.var(window, axis=1),\n",
    "#                 kurtosis(window, axis=1),\n",
    "#                 skew(window, axis=1),\n",
    "            ])\n",
    "        if features:\n",
    "            return np.vstack(features).T  # Stack vertically and transpose to get the correct shape\n",
    "        else:\n",
    "            return np.empty((data.shape[0], 0))  # Return empty array with correct shape\n",
    "    \n",
    "    f_sliding_features = sliding_window_features(f_raw, window_size, step_size)\n",
    "    a_sliding_features = sliding_window_features(a_raw, window_size, step_size)\n",
    "    \n",
    "    f_sliding_features2 = sliding_window_features(f_raw, 150, 10)\n",
    "    a_sliding_features2 = sliding_window_features(a_raw, 150, 10)\n",
    "\n",
    "\n",
    "    print(f'f_sliding_features.shape: {f_sliding_features.shape}')\n",
    "    print(f'a_sliding_features.shape: {a_sliding_features.shape}')\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'f_relative_reduction': f_relative_reduction,\n",
    "        'f_signal_to_noise': f_signal_to_noise,\n",
    "        'f_variance': f_variance,\n",
    "        'f_skewness': f_skewness,\n",
    "        'f_kurtosis': f_kurtosis,\n",
    "        'a_relative_reduction': a_relative_reduction,\n",
    "        'a_signal_to_noise': a_signal_to_noise,\n",
    "        'a_variance': a_variance,\n",
    "        'a_skewness': a_skewness,\n",
    "        'a_kurtosis': a_kurtosis,\n",
    "        'f_half_reduction1': f_half_reduction1,\n",
    "        'f_half_reduction2': f_half_reduction2,\n",
    "        'a_half_reduction1': a_half_reduction1,\n",
    "        'a_half_reduction2': a_half_reduction2\n",
    "    })\n",
    "\n",
    "\n",
    "    if f_sliding_features.size > 0:\n",
    "        f_sliding_df = pd.DataFrame(f_sliding_features, columns=[f'f_slide_{i}' for i in range(f_sliding_features.shape[1])])\n",
    "        df = pd.concat([df, f_sliding_df], axis=1)\n",
    "    if a_sliding_features.size > 0:\n",
    "        a_sliding_df = pd.DataFrame(a_sliding_features, columns=[f'a_slide_{i}' for i in range(a_sliding_features.shape[1])])\n",
    "        df = pd.concat([df, a_sliding_df], axis=1)\n",
    "    \n",
    "    if f_sliding_features2.size > 0:\n",
    "        f_sliding_df = pd.DataFrame(f_sliding_features2, columns=[f'f_slide2_{i}' for i in range(f_sliding_features2.shape[1])])\n",
    "        df = pd.concat([df, f_sliding_df], axis=1)\n",
    "    if a_sliding_features2.size > 0:\n",
    "        a_sliding_df = pd.DataFrame(a_sliding_features2, columns=[f'a_slide2_{i}' for i in range(a_sliding_features2.shape[1])])\n",
    "        df = pd.concat([df, a_sliding_df], axis=1)\n",
    "    \n",
    "    \n",
    "    df = pd.concat([df, adc_info.reset_index().iloc[:, 1:6]], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:29:51.561369Z",
     "iopub.status.busy": "2024-12-28T10:29:51.560936Z",
     "iopub.status.idle": "2024-12-28T10:29:51.749084Z",
     "shell.execute_reply": "2024-12-28T10:29:51.747394Z",
     "shell.execute_reply.started": "2024-12-28T10:29:51.561329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Meta-Data\n",
    "PATH = \"/kaggle/input/ariel-data-challenge-2024\"\n",
    "train_adc_info = pd.read_csv('/kaggle/input/ariel-data-challenge-2024/train_adc_info.csv', \n",
    "                             index_col='planet_id')\n",
    "train_labels = pd.read_csv('/kaggle/input/ariel-data-challenge-2024/train_labels.csv',\n",
    "                           index_col='planet_id')\n",
    "wavelengths = pd.read_csv(f'{PATH}/wavelengths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:29:51.751353Z",
     "iopub.status.busy": "2024-12-28T10:29:51.750814Z",
     "iopub.status.idle": "2024-12-28T10:29:51.760553Z",
     "shell.execute_reply": "2024-12-28T10:29:51.759202Z",
     "shell.execute_reply.started": "2024-12-28T10:29:51.751310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "exec(open('utils.py', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:29:51.763185Z",
     "iopub.status.busy": "2024-12-28T10:29:51.762622Z",
     "iopub.status.idle": "2024-12-28T10:29:51.775323Z",
     "shell.execute_reply": "2024-12-28T10:29:51.773914Z",
     "shell.execute_reply.started": "2024-12-28T10:29:51.763152Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a utils.py\n",
    "\n",
    "def postprocessing(pred_array, index, sigma_pred):\n",
    "    \"\"\"Create a submission dataframe from its components\n",
    "    \n",
    "    Parameters:\n",
    "    pred_array: ndarray of shape (n_samples, 283)\n",
    "    index: pandas.Index of length n_samples with name 'planet_id'\n",
    "    sigma_pred: float\n",
    "    \n",
    "    Return value:\n",
    "    df: DataFrame of shape (n_samples, 566) with planet_id as index\n",
    "    \"\"\"\n",
    "    return pd.concat([pd.DataFrame(pred_array.clip(0, None), index=index, columns=wavelengths.columns),\n",
    "                      pd.DataFrame(sigma_pred, index=index, columns=[f\"sigma_{i}\" for i in range(1, 284)])],\n",
    "                     axis=1)\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def competition_score(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        naive_mean: float,\n",
    "        naive_sigma: float,\n",
    "        sigma_true: float,\n",
    "        row_id_column_name='planet_id',\n",
    "    ) -> float:\n",
    "    '''\n",
    "    This is a Gaussian Log Likelihood based metric. For a submission, which contains the predicted mean (x_hat) and variance (x_hat_std),\n",
    "    we calculate the Gaussian Log-likelihood (GLL) value to the provided ground truth (x). We treat each pair of x_hat,\n",
    "    x_hat_std as a 1D gaussian, meaning there will be 283 1D gaussian distributions, hence 283 values for each test spectrum,\n",
    "    the GLL value for one spectrum is the sum of all of them.\n",
    "\n",
    "    Inputs:\n",
    "        - solution: Ground Truth spectra (from test set)\n",
    "            - shape: (nsamples, n_wavelengths)\n",
    "        - submission: Predicted spectra and errors (from participants)\n",
    "            - shape: (nsamples, n_wavelengths*2)\n",
    "        naive_mean: (float) mean from the train set.\n",
    "        naive_sigma: (float) standard deviation from the train set.\n",
    "        sigma_true: (float) essentially sets the scale of the outputs.\n",
    "    '''\n",
    "\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "\n",
    "    if submission.min().min() < 0:\n",
    "        raise ParticipantVisibleError('Negative values in the submission')\n",
    "    for col in submission.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "\n",
    "    n_wavelengths = len(solution.columns)\n",
    "    if len(submission.columns) != n_wavelengths*2:\n",
    "        raise ParticipantVisibleError('Wrong number of columns in the submission')\n",
    "\n",
    "    y_pred = submission.iloc[:, :n_wavelengths].values\n",
    "    # Set a non-zero minimum sigma pred to prevent division by zero errors.\n",
    "    sigma_pred = np.clip(submission.iloc[:, n_wavelengths:].values, a_min=10**-15, a_max=None)\n",
    "    y_true = solution.values\n",
    "\n",
    "    GLL_pred = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_pred, scale=sigma_pred))\n",
    "    GLL_true = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_true, scale=sigma_true * np.ones_like(y_true)))\n",
    "    GLL_mean = np.sum(scipy.stats.norm.logpdf(y_true, loc=naive_mean * np.ones_like(y_true), scale=naive_sigma * np.ones_like(y_true)))\n",
    "\n",
    "    submit_score = (GLL_pred - GLL_mean)/(GLL_true - GLL_mean)\n",
    "    return float(np.clip(submit_score, 0.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:29:51.777346Z",
     "iopub.status.busy": "2024-12-28T10:29:51.776844Z",
     "iopub.status.idle": "2024-12-28T10:29:53.663886Z",
     "shell.execute_reply": "2024-12-28T10:29:53.662674Z",
     "shell.execute_reply.started": "2024-12-28T10:29:51.777305Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 204 ms, total: 204 ms\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(\"/kaggle/input/adc24-intro-training/f_raw_train.pickle\"):\n",
    "    f_raw_train = np.load('/kaggle/input/adc24-intro-training/f_raw_train.pickle', allow_pickle=True)\n",
    "else:\n",
    "    f_raw_train = read_and_preprocess('train', train_labels.index, 'FGS1')\n",
    "    with open('f_raw_train.pickle', 'wb') as f:\n",
    "        pickle.dump(f_raw_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:29:53.668008Z",
     "iopub.status.busy": "2024-12-28T10:29:53.667412Z",
     "iopub.status.idle": "2024-12-28T10:29:53.779945Z",
     "shell.execute_reply": "2024-12-28T10:29:53.778805Z",
     "shell.execute_reply.started": "2024-12-28T10:29:53.667977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.74 ms, sys: 13.6 ms, total: 19.4 ms\n",
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(\"/kaggle/input/adc24-intro-training/a_raw_train.pickle\"):\n",
    "    a_raw_train = np.load('/kaggle/input/adc24-intro-training/a_raw_train.pickle', allow_pickle=True)\n",
    "else:\n",
    "    a_raw_train = read_and_preprocess('train', train_labels.index)\n",
    "    with open('a_raw_train.pickle', 'wb') as f:\n",
    "        pickle.dump(a_raw_train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:29:53.781465Z",
     "iopub.status.busy": "2024-12-28T10:29:53.781175Z",
     "iopub.status.idle": "2024-12-28T10:35:50.056483Z",
     "shell.execute_reply": "2024-12-28T10:35:50.055146Z",
     "shell.execute_reply.started": "2024-12-28T10:29:53.781441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_sliding_features.shape: (673, 98934)\n",
      "a_sliding_features.shape: (673, 8184)\n",
      "CPU times: user 5min 49s, sys: 6.32 s, total: 5min 55s\n",
      "Wall time: 5min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = feature_engineering(f_raw_train, a_raw_train, train_adc_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:35:50.058021Z",
     "iopub.status.busy": "2024-12-28T10:35:50.057663Z",
     "iopub.status.idle": "2024-12-28T10:35:50.064788Z",
     "shell.execute_reply": "2024-12-28T10:35:50.063067Z",
     "shell.execute_reply.started": "2024-12-28T10:35:50.057993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673, 267385)\n"
     ]
    }
   ],
   "source": [
    "train.head()\n",
    "print(train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:35:50.066671Z",
     "iopub.status.busy": "2024-12-28T10:35:50.066339Z",
     "iopub.status.idle": "2024-12-28T10:35:50.081036Z",
     "shell.execute_reply": "2024-12-28T10:35:50.079658Z",
     "shell.execute_reply.started": "2024-12-28T10:35:50.066642Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673, 283)\n"
     ]
    }
   ],
   "source": [
    "train_labels.head()\n",
    "\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:35:50.083266Z",
     "iopub.status.busy": "2024-12-28T10:35:50.082808Z",
     "iopub.status.idle": "2024-12-28T10:35:50.565090Z",
     "shell.execute_reply": "2024-12-28T10:35:50.563878Z",
     "shell.execute_reply.started": "2024-12-28T10:35:50.083216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = train.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:35:50.588676Z",
     "iopub.status.busy": "2024-12-28T10:35:50.587832Z",
     "iopub.status.idle": "2024-12-28T10:35:50.602073Z",
     "shell.execute_reply": "2024-12-28T10:35:50.600799Z",
     "shell.execute_reply.started": "2024-12-28T10:35:50.588633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (471, 267384)\n",
      "test_data.shape: (202, 267384)\n",
      "train_labels_split.shape: (471, 283)\n",
      "test_labels_split.shape: (202, 283)\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集和测试集\n",
    "train_data = train.iloc[:471]   # 前 471 行作为训练集\n",
    "test_data = train.iloc[471:]    # 后 202 行作为测试集\n",
    "\n",
    "train_labels_split = train_labels.iloc[:471]  # 对应的训练标签\n",
    "test_labels_split = train_labels.iloc[471:]  # 对应的测试标签\n",
    "\n",
    "# 打印数据形状，确保划分正确\n",
    "print(f\"train_data.shape: {train_data.shape}\")   # 应为 (471, 267384)\n",
    "print(f\"test_data.shape: {test_data.shape}\")     # 应为 (202, 267384)\n",
    "print(f\"train_labels_split.shape: {train_labels_split.shape}\")  # 应为 (471,)\n",
    "print(f\"test_labels_split.shape: {test_labels_split.shape}\")    # 应为 (202,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:35:50.603932Z",
     "iopub.status.busy": "2024-12-28T10:35:50.603549Z",
     "iopub.status.idle": "2024-12-28T10:36:22.414599Z",
     "shell.execute_reply": "2024-12-28T10:36:22.412728Z",
     "shell.execute_reply.started": "2024-12-28T10:35:50.603900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_scaled shape: (202, 267384)\n",
      "训练集 PCA 结果形状: (471, 200)\n",
      "测试集 PCA 结果形状: (202, 200)\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 假设 train 是一个 pandas DataFrame，每行代表一个行星，每列代表一个特征\n",
    "\n",
    "# 检查缺失值并填补\n",
    "if train_data.isnull().values.any():\n",
    "    train_data = train_data.fillna(train_data.mean())\n",
    "if test_data.isnull().values.any():\n",
    "    test_data = test_data.fillna(test_data.mean())\n",
    "    \n",
    "# 分离特征\n",
    "X_train = train_data.values  # 或者 train_data.to_numpy()\n",
    "X_test = test_data.values    # 或者 test_data.to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 初始化 PCA，保留 200 个主成分\n",
    "pca = PCA(n_components=200, random_state=42)\n",
    "\n",
    "# 拟合 PCA 并转换训练数据\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# 创建训练集主成分的列名\n",
    "pca_columns = [f'PC{i}' for i in range(1, X_train_pca.shape[1] + 1)]\n",
    "\n",
    "# 转换为 DataFrame\n",
    "train_pca_df = pd.DataFrame(X_train_pca, index=train_data.index, columns=pca_columns)\n",
    "\n",
    "# 对测试集进行标准化，并转换到相同的 PCA 空间\n",
    "X_test_scaled = scaler.transform(X_test)  # 使用训练集的标准化参数\n",
    "\n",
    "# 确保测试集是二维数据\n",
    "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")  # 应为 (202, 267384)\n",
    "\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# 创建测试集主成分的列名\n",
    "test_pca_columns = [f'PC{i}' for i in range(1, X_test_pca.shape[1] + 1)]\n",
    "\n",
    "# 转换为 DataFrame\n",
    "test_pca_df = pd.DataFrame(X_test_pca, index=test_data.index, columns=test_pca_columns)\n",
    "\n",
    "# 查看降维后的训练集和测试集数据形状\n",
    "print(f\"训练集 PCA 结果形状: {train_pca_df.shape}\")\n",
    "print(f\"测试集 PCA 结果形状: {test_pca_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:36:22.416449Z",
     "iopub.status.busy": "2024-12-28T10:36:22.416003Z",
     "iopub.status.idle": "2024-12-28T10:36:24.242068Z",
     "shell.execute_reply": "2024-12-28T10:36:24.240746Z",
     "shell.execute_reply.started": "2024-12-28T10:36:22.416412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 保存Scaler和PCA模型\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('pca_model.pkl', 'wb') as f:\n",
    "    pickle.dump(pca, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "## Xgboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:36:24.244319Z",
     "iopub.status.busy": "2024-12-28T10:36:24.243849Z",
     "iopub.status.idle": "2024-12-28T10:37:29.370623Z",
     "shell.execute_reply": "2024-12-28T10:37:29.369002Z",
     "shell.execute_reply.started": "2024-12-28T10:36:24.244280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 283)\n",
      "# R2 score on test set: 0.4686\n",
      "# Root Mean Squared Error on test set: 0.0012806\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# print(train.shape)\n",
    "# print(train)\n",
    "    \n",
    "model = XGBRegressor(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "        n_jobs=-1\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_pca_df, train_labels_split)\n",
    "\n",
    "# 对测试集进行预测\n",
    "test_pred = model.predict(test_pca_df)\n",
    "print(test_pred.shape)\n",
    "\n",
    "# 测试集评估：计算 R2 和 RMSE\n",
    "print(f\"# R2 score on test set: {r2_score(test_labels_split, test_pred):.4f}\")\n",
    "sigma_pred_test = mean_squared_error(test_labels_split, test_pred, squared=False)\n",
    "print(f\"# Root Mean Squared Error on test set: {sigma_pred_test:.7f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:44:20.177520Z",
     "iopub.status.busy": "2024-12-28T10:44:20.177143Z",
     "iopub.status.idle": "2024-12-28T10:44:20.194802Z",
     "shell.execute_reply": "2024-12-28T10:44:20.193531Z",
     "shell.execute_reply.started": "2024-12-28T10:44:20.177493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_ref_mean =  4.925\n",
      "L_ideal =  10.594\n",
      "L =  6.089\n"
     ]
    }
   ],
   "source": [
    "# GLL\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calculate_gll(y_pred=0,y_true=0,sigmas=1e-5):\n",
    "    var = sigmas**2\n",
    "    mse = (y_pred - y_true)**2\n",
    "    score = -0.5*(np.log(2*math.pi)+np.log(var)+mse/var)\n",
    "    return round(score.mean(),3)\n",
    "\n",
    "\n",
    "# L_ref\n",
    "mn = np.mean(test_labels_split.values, axis=0, keepdims=True)\n",
    "stdev = np.std(test_labels_split.values, axis=0, keepdims=True)\n",
    "L_ref = calculate_gll(mn,test_labels_split,stdev)\n",
    "L_ref_mean = round(L_ref.mean(), 3)\n",
    "print(\"L_ref_mean = \",L_ref_mean)\n",
    "\n",
    "# L_ideal\n",
    "L_ideal = calculate_gll()\n",
    "L_ideal_mean = round(L_ideal.mean(),3)\n",
    "print(\"L_ideal = \",L_ideal_mean)\n",
    "\n",
    "# L\n",
    "mn_test = np.mean(test_pred, axis=0, keepdims=True)\n",
    "stdev_test = np.std(test_pred, axis=0, keepdims=True)\n",
    "L = calculate_gll(mn_test, test_pred, stdev_test)\n",
    "L_mean = round(L.mean(), 3)\n",
    "print(\"L = \",L_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:44:26.200940Z",
     "iopub.status.busy": "2024-12-28T10:44:26.199936Z",
     "iopub.status.idle": "2024-12-28T10:44:26.207073Z",
     "shell.execute_reply": "2024-12-28T10:44:26.205725Z",
     "shell.execute_reply.started": "2024-12-28T10:44:26.200894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score =  0.2053272579056516\n"
     ]
    }
   ],
   "source": [
    "# Score\n",
    "score = (L_mean - L_ref_mean)/(L_ideal_mean - L_ref_mean)\n",
    "print(\"Score = \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-28T10:37:29.615136Z",
     "iopub.status.idle": "2024-12-28T10:37:29.615539Z",
     "shell.execute_reply": "2024-12-28T10:37:29.615378Z",
     "shell.execute_reply.started": "2024-12-28T10:37:29.615361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_pred_df = pd.DataFrame(test_pred, columns=[f\"Feature_{i+1}\" for i in range(test_pred.shape[1])])\n",
    "test_pred_df.columns = test_labels_split.columns\n",
    "# test_pred_df.index = test_labels_split.index\n",
    "\n",
    "\n",
    "print(\"True\")\n",
    "print(test_labels_split.head())\n",
    "\n",
    "print(\"Predict\")\n",
    "print(test_pred_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-28T10:37:29.616806Z",
     "iopub.status.idle": "2024-12-28T10:37:29.617157Z",
     "shell.execute_reply": "2024-12-28T10:37:29.617013Z",
     "shell.execute_reply.started": "2024-12-28T10:37:29.616998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 提取前10个星球的真实值和预测值\n",
    "test_labels_split_top10 = test_labels_split.iloc[:10, :]\n",
    "test_pred_df_top10 = test_pred_df.iloc[:10, :]\n",
    "\n",
    "# 设置画布大小\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 15))  # 5行2列，共十张图\n",
    "\n",
    "# 遍历前10个星球\n",
    "for i in range(10):\n",
    "    # 选择第i个星球的真实值和预测值\n",
    "    true_values = test_labels_split_top10.iloc[i, :]\n",
    "    predicted_values = test_pred_df_top10.iloc[i, :]\n",
    "\n",
    "    # 计算当前子图位置\n",
    "    ax = axes[i // 2, i % 2]\n",
    "\n",
    "    # 画出真实值和预测值\n",
    "    ax.plot(range(283), true_values, label=\"True Values\", color='blue', linestyle='-')\n",
    "    ax.plot(range(283), predicted_values, label=\"Predicted Values\", color='red', linestyle='-')\n",
    "\n",
    "    # 设置标题和标签\n",
    "    ax.set_title(f\"Planet {i + 1}\")\n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "    ax.set_ylim(0, 0.05)\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "# 调整子图之间的间距\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9188054,
     "sourceId": 70367,
     "sourceType": "competition"
    },
    {
     "datasetId": 5492689,
     "sourceId": 9101333,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 191155259,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
